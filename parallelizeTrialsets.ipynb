{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9f461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: must use a microsim kernel or a kernel that can load all necessary python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554af5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52aca676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsim.alcohol_category import AlcoholCategory\n",
    "from microsim.bp_treatment_strategies import *\n",
    "from microsim.cohort_risk_model_repository import CohortRiskModelRepository\n",
    "from microsim.cv_outcome_determination import CVOutcomeDetermination\n",
    "from microsim.data_loader import (get_absolute_datafile_path,\n",
    "                                  load_regression_model)\n",
    "from microsim.education import Education\n",
    "from microsim.gender import NHANESGender\n",
    "from microsim.gfr_equation import GFREquation\n",
    "from microsim.initialization_repository import InitializationRepository\n",
    "from microsim.nhanes_risk_model_repository import NHANESRiskModelRepository\n",
    "from microsim.outcome import Outcome, OutcomeType\n",
    "from microsim.outcome_model_repository import OutcomeModelRepository\n",
    "from microsim.outcome_model_type import OutcomeModelType\n",
    "from microsim.person import Person\n",
    "from microsim.qaly_assignment_strategy import QALYAssignmentStrategy\n",
    "from microsim.race_ethnicity import NHANESRaceEthnicity\n",
    "from microsim.smoking_status import SmokingStatus\n",
    "from microsim.statsmodel_logistic_risk_factor_model import \\\n",
    "    StatsModelLogisticRiskFactorModel\n",
    "\n",
    "from microsim.population import NHANESDirectSamplePopulation\n",
    "#from microsim.trials.trial_description import TrialDescription\n",
    "from microsim.trials.trial import Trial\n",
    "from microsim.outcome_model_type import OutcomeModelType\n",
    "from microsim.bp_treatment_strategies import SprintTreatment\n",
    "from microsim.trials.logistic_regression_analysis import LogisticRegressionAnalysis\n",
    "from microsim.trials.linear_regression_analysis import LinearRegressionAnalysis\n",
    "from microsim.trials.outcome_assessor import OutcomeAssessor\n",
    "from microsim.trials.attribute_outcome_assessor import AttributeOutcomeAssessor\n",
    "from microsim.trials.attribute_outcome_assessor import AssessmentMethod\n",
    "from microsim.outcome import OutcomeType\n",
    "from microsim.trials.risk_filter import RiskFilter\n",
    "from microsim.trials.trial_utils import get_analysis_name\n",
    "from microsim.population import PersonListPopulation\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ce8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#any microsim dir will work, just need to access the NHANES data\n",
    "microsimDir = \"/users/PAS2164/deligkaris/MICROSIM/CODE/microsim\"\n",
    "os.chdir(microsimDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249bd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I had to disable pandarallel on this class which is why I copied the entire population.py file here \n",
    "#in method get_people_current_state_as_dataframe the variable parallel is used \n",
    "#to enable/disable pandarallel but parallel can mean other things as well\n",
    "#for now I used variable pandarallelOn elsewhere\n",
    "#you can probably safely skip this entire code box\n",
    "\n",
    "class Population:\n",
    "    \"\"\"\n",
    "    Unit of people subject to treatment program over time.\n",
    "\n",
    "    (WIP) THe basic idea is that this is a generic superclass which will manage a group of\n",
    "    people. Tangible subclasses will be needed to actually assign assumptions for a given\n",
    "    population. As it stands, this class doesn't do anything (other than potentially being\n",
    "    useful for tests), because it isn't tied to tangible risk models. Ultimately it might\n",
    "    turn into an abstract class...\n",
    "    \"\"\"\n",
    "\n",
    "    _ageStandards = {}\n",
    "\n",
    "    def __init__(self, people):\n",
    "        self._people = people\n",
    "        self._ageStandards = {}\n",
    "        # luciana tag: discuss with luciana...want to keep track of the sim wave htat is currently running, while running\n",
    "        # and also the total number of years advanced...need to think about how to do this is a way that will be safe\n",
    "        # this approach has major risks if you forget to update one of these variables\n",
    "        self._totalWavesAdvanced = 0\n",
    "        self._currentWave = 0\n",
    "        self._bpTreatmentStrategy = None\n",
    "        self.num_of_processes = 8\n",
    "\n",
    "        self._riskFactors = [\n",
    "            \"sbp\",\n",
    "            \"dbp\",\n",
    "            \"a1c\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"trig\",\n",
    "            \"totChol\",\n",
    "            \"bmi\",\n",
    "            \"anyPhysicalActivity\",\n",
    "            \"afib\",\n",
    "            \"waist\",\n",
    "            \"alcoholPerWeek\",\n",
    "            \"creatinine\",\n",
    "        ]\n",
    "        # , 'otherLipidLoweringMedicationCount']\n",
    "        self._treatments = [\"antiHypertensiveCount\", \"statin\"]\n",
    "        self._timeVaryingCovariates = copy.copy(self._riskFactors)\n",
    "        self._timeVaryingCovariates.append(\"age\")\n",
    "        self._timeVaryingCovariates.extend(self._treatments)\n",
    "        self._timeVaryingCovariates.append(\"bpMedsAdded\")\n",
    "\n",
    "    def reset_to_baseline(self):\n",
    "        self._totalWavesAdvanced = 0\n",
    "        self._currentWave = 0\n",
    "        self._bpTreatmentStrategy = None\n",
    "        for person in self._people:\n",
    "            person.reset_to_baseline()\n",
    "\n",
    "    # trying to work this out. if we do get it worked out, then we probably want to rebuild the person to use systematic data structrures\n",
    "    # (i.e. static attributes, time-varying attributes)\n",
    "    # also, will need to thikn about ways to make sure that the dataframe version of reality stays synced with teh \"patient-based\" version of reality\n",
    "    # for now, will build a DF at the beginnign and then update the peopel at the end...\n",
    "    def advance_vectorized(self, years):\n",
    "        pandarallelOn = False #CD added\n",
    "        # get dataframe of people...\n",
    "        df = self.get_people_current_state_and_summary_as_dataframe()\n",
    "        alive = df.loc[df.dead == False]\n",
    "        if pandarallelOn is True: #CD added\n",
    "            pandarallel.initialize(verbose=1) \n",
    "        # might not need this row...depends o n whethe we do an bulk update on people or an wave-abased update\n",
    "        waveAtStartOfAdvance = self._currentWave\n",
    "\n",
    "        for yearIndex in range(years):\n",
    "            logging.info(f\"processing year: {self._currentWave}\")\n",
    "            alive = alive.loc[alive.dead == False]\n",
    "            # if everybody has died, break out of the loop, no need to keep moving forward\n",
    "            if len(alive) == 0:\n",
    "                break\n",
    "            self._currentWave += 1\n",
    "\n",
    "            riskFactorsAndTreatment = {}\n",
    "            # advance risk factors\n",
    "            #import pdb; pdb.set_trace()\n",
    "            for rf in self._riskFactors:\n",
    "                # print(f\"### Risk Factor: {rf}\")\n",
    "                if pandarallelOn is True: #CD added\n",
    "                    riskFactorsAndTreatment[rf + \"Next\"] = alive.parallel_apply(\n",
    "                        self._risk_model_repository.get_model(rf).estimate_next_risk_vectorized,\n",
    "                        axis=\"columns\",\n",
    "                    )\n",
    "                else: #CD added\n",
    "                    riskFactorsAndTreatment[rf + \"Next\"] = alive.apply(\n",
    "                        self._risk_model_repository.get_model(rf).estimate_next_risk_vectorized,\n",
    "                        axis=\"columns\",\n",
    "                    )\n",
    "\n",
    "            # advance treatment\n",
    "            for treatment in self._treatments:\n",
    "                # print(f\"### Treatment: {treatment}\")\n",
    "                riskFactorsAndTreatment[treatment + \"Next\"] = alive.apply(\n",
    "                    self._risk_model_repository.get_model(treatment).estimate_next_risk_vectorized,\n",
    "                    axis=\"columns\",\n",
    "                )\n",
    "\n",
    "            # apply treatment modifications\n",
    "            alive = pd.concat([alive.reset_index(drop=True), pd.DataFrame(riskFactorsAndTreatment).reset_index(drop=True)], axis='columns', ignore_index=False)\n",
    "            \n",
    "            # bp meds added in this wave are 0...\n",
    "            medsData = pd.DataFrame({'bpMedsAddedNext' :  pd.Series(np.zeros(len(alive))), \n",
    "                                    'totalBPMedsAddedNext' : pd.Series(alive['totalBPMedsAdded'])})\n",
    "            \n",
    "            # total bp meds are carried forward from teh prior wave\n",
    "            alive = pd.concat([alive.reset_index(drop=True), medsData], axis='columns', ignore_index=False)\n",
    "            if self._bpTreatmentStrategy is not None:\n",
    "                alive = alive.apply(\n",
    "                    self._bpTreatmentStrategy.get_changes_vectorized, axis=\"columns\"\n",
    "                )\n",
    "            # advance outcomes\n",
    "            # first determine if there is a cv event\n",
    "            # add these variables here to speed up performance...better than adding one at a time\n",
    "            # in the advance method...\n",
    "            outcomeVars = {}\n",
    "            # first, setup outcome variables\n",
    "            for outcome in [\"stroke\", \"mi\", \"dementia\", \"dead\", \"cvDeath\", 'nonCVDeath']:\n",
    "                outcomeVars[outcome + \"Next\"] = [False] * len(alive)\n",
    "            outcomeVars[\"strokeFatal\"] = [False] * len(alive)\n",
    "            outcomeVars[\"miFatal\"] =[False] * len(alive)\n",
    "            #outcomeVars[\"qalyNext\"] =np.zeros(len(alive))\n",
    "            #outcomeVars[\"ageAtFirstDementia\"] = [np.nan] * len(df)\n",
    "            alive = pd.concat([alive.reset_index(drop=True), pd.DataFrame(outcomeVars) ], axis='columns')\n",
    "            alive = alive.apply(\n",
    "                self._outcome_model_repository.assign_cv_outcome_vectorized, axis=\"columns\"\n",
    "            )\n",
    "\n",
    "            gcp = {}\n",
    "            gcp[\"gcpNext\"] = alive.apply(\n",
    "                self._outcome_model_repository.get_gcp_vectorized, axis=\"columns\"\n",
    "            )\n",
    "            gcp[\"gcpSlope\"] = gcp['gcpNext'] - alive['gcp']\n",
    "            alive.drop(columns=['gcpSlope'], inplace=True)\n",
    "            alive = pd.concat([alive.reset_index(drop=True), pd.DataFrame(gcp)], axis='columns')\n",
    "\n",
    "            # if the whole popluation has demeentia, nobody new can get dementia...            \n",
    "            alive.dementia = alive.dementia.astype('bool_')\n",
    "            if len(alive.loc[~alive.dementia]) > 0:\n",
    "                newDementia = alive.loc[~alive.dementia].apply(\n",
    "                    self._outcome_model_repository.get_dementia_vectorized, axis=\"columns\"\n",
    "                )\n",
    "                alive[\"dementiaNext\"] = newDementia\n",
    "            else:\n",
    "                alive[\"dementiaNext\"] = np.repeat(False, len(alive))\n",
    "\n",
    "            alive.loc[alive[\"dementiaNext\"] == 1, \"ageAtFirstDementia\"] = alive.age\n",
    "            alive[\"dementia\"] = newDementia | alive[\"dementia\"]\n",
    "\n",
    "            numberAliveBeforeRecal = len(alive)\n",
    "\n",
    "            alive = self.apply_recalibration_standards(alive)\n",
    "            if len(alive) != numberAliveBeforeRecal:\n",
    "                raise Exception(\n",
    "                    f\"number alive: {len(alive)} not equal to alive before recal {numberAliveBeforeRecal}\"\n",
    "                )\n",
    "            alive[\"cvDeathNext\"] = alive[\"deadNext\"]\n",
    "            alive[\"nonCVDeathNext\"] = alive.apply(\n",
    "                self._outcome_model_repository.assign_non_cv_mortality_vectorized, axis=\"columns\"\n",
    "            )\n",
    "            alive[\"deadNext\"] = alive[\"nonCVDeathNext\"] | alive[\"cvDeathNext\"]\n",
    "\n",
    "            qaly = {}\n",
    "            qaly[\"qalyNext\"] = alive.apply(\n",
    "                QALYAssignmentStrategy().get_qalys_vectorized, axis=\"columns\"\n",
    "            )\n",
    "            alive = pd.concat([alive.reset_index(drop=True), pd.DataFrame(qaly)], axis='columns')\n",
    "\n",
    "            alive.loc[~alive.dead, \"age\"] = alive.age + 1\n",
    "            self._totalWavesAdvanced += 1\n",
    "\n",
    "            alive = self.move_people_df_forward(alive)\n",
    "\n",
    "            # for efficieicny, we could try to do this all at the end...but, its a bit cleanear  to do it wave by wave\n",
    "            alive.apply(self.push_updates_back_to_people, axis=\"columns\")\n",
    "            nextCols = [col for col in alive.columns if \"Next\" in col]\n",
    "            alive.drop(columns=nextCols, inplace=True)\n",
    "            fatalCols = [col for col in alive.columns if \"Fatal\" in col]\n",
    "            alive.drop(columns=fatalCols, inplace=True)\n",
    "        return alive, df\n",
    "\n",
    "    def push_updates_back_to_people(self, x):\n",
    "        updatedIndices = set()\n",
    "        peopleSet = set()\n",
    "        person = self._people.iloc[int(x.populationIndex)]\n",
    "        if x.populationIndex in updatedIndices or person in peopleSet:\n",
    "            raise Exception(f\"Population index: {x.populationIndex} already updated\")\n",
    "        else:\n",
    "            updatedIndices.add(x.populationIndex)\n",
    "            peopleSet.add(person)\n",
    "        return self.update_person(person, x)\n",
    "\n",
    "    def update_person(self, person, x):\n",
    "        if person.is_dead():\n",
    "            raise Exception(f\"Trying to update a dead person: {person}\")\n",
    "        for rf in self._riskFactors:\n",
    "            attr = getattr(person, \"_\" + rf)\n",
    "            attr.append(x[rf + str(self._currentWave)])\n",
    "\n",
    "        for treatment in self._treatments:\n",
    "            attr = getattr(person, \"_\" + treatment)\n",
    "            attr.append(x[treatment + str(self._currentWave)])\n",
    "\n",
    "        # advance outcomes - this will add CV eath\n",
    "        for outcomeName, outcomeType in {\n",
    "            \"stroke\": OutcomeType.STROKE,\n",
    "            \"mi\": OutcomeType.MI,\n",
    "            \"dementia\": OutcomeType.DEMENTIA,\n",
    "        }.items():\n",
    "            if x[outcomeName + \"Next\"]:\n",
    "                fatal = False if outcomeName == \"dementia\" else x[outcomeName + \"Fatal\"]\n",
    "                # only one dementia event per person\n",
    "                if outcomeName == \"dementia\" and person._dementia:\n",
    "                    break\n",
    "                else:\n",
    "                    person.add_outcome_event(Outcome(outcomeType, fatal))\n",
    "\n",
    "        person._gcp.append(x.gcp)\n",
    "        person._qalys.append(x.qalyNext)\n",
    "        person._bpMedsAdded.append(x.bpMedsAddedNext)\n",
    "\n",
    "        # add non CV death to person objects\n",
    "        if x.nonCVDeathNext:\n",
    "            person._alive.append(False)\n",
    "\n",
    "        # only advance age in survivors\n",
    "        if not x.deadNext:\n",
    "            person._age.append(person._age[-1] + 1)\n",
    "            person._alive.append(True)\n",
    "        return person\n",
    "\n",
    "    def move_people_df_forward(self, df):\n",
    "        factorsToChange = copy.copy(self._riskFactors)\n",
    "        factorsToChange.extend(self._treatments)\n",
    "        \n",
    "        newVariables = {}\n",
    "        \n",
    "        for rf in factorsToChange:\n",
    "            # the curent value is stored in the variable name\n",
    "            df[rf] = df[rf + \"Next\"]\n",
    "            newVariables[rf + str(self._currentWave)] = df[rf + \"Next\"]\n",
    "            df[\"mean\" + rf.capitalize()] = (\n",
    "                df[\"mean\" + rf.capitalize()] * (df[\"totalYearsInSim\"] + 1) + df[rf + \"Next\"]\n",
    "            ) / (df[\"totalYearsInSim\"] + 2)\n",
    "        for outcome in [\"mi\", \"stroke\"]:\n",
    "            df[outcome + \"InSim\"] = df[outcome + \"InSim\"] | df[outcome + \"Next\"]\n",
    "            newVariables[outcome + str(self._currentWave)] = df[outcome + \"Next\"]\n",
    "        df[\"dead\"] = df[\"dead\"] | df[\"deadNext\"]\n",
    "        newVariables[\"dead\" + str(self._currentWave)] = df[\"deadNext\"]\n",
    "\n",
    "        df[\"totalYearsInSim\"] = df[\"totalYearsInSim\"] + 1\n",
    "        df[\"current_diabetes\"] = df[\"a1c\"] > 6.5\n",
    "        df['gcp'] = df['gcpNext']\n",
    "        df[\"gfr\"] = df.apply(GFREquation().get_gfr_for_person_vectorized, axis=\"columns\")\n",
    "        df[\"current_bp_treatment\"] = df[\"antiHypertensiveCount\"] >= 1\n",
    "        df[\"totalQalys\"] = df[\"totalQalys\"] + df[\"qalyNext\"]\n",
    "        df[\"bpMedsAdded\"] = df[\"bpMedsAddedNext\"]\n",
    "        df[\"totalBPMedsAdded\"] = df[\"totalBPMedsAddedNext\"]\n",
    "        \n",
    "        # assign ages for new events\n",
    "        # df.loc[(df.ageAtFirstStroke.isnull()) & (df.strokeNext), 'ageAtFirstStroke'] = df.age\n",
    "        # df.loc[(df.ageAtFirstMI.isnull()) & (df.miNext), 'ageAtFirstMI'] = df.age\n",
    "        # df.loc[(df.ageAtFirstDementia.isnull()) & (df.dementiaNext), 'ageAtFirstDementia'] = df.age\n",
    "\n",
    "        return pd.concat([df.reset_index(drop=True), pd.DataFrame(newVariables).reset_index(drop=True)], axis='columns', ignore_index=False)\n",
    "\n",
    "\n",
    "    def set_bp_treatment_strategy(self, bpTreatmentStrategy):\n",
    "        self._bpTreatmentStrategy = bpTreatmentStrategy\n",
    "        for person in self._people:\n",
    "            person._bpTreatmentStrategy = bpTreatmentStrategy\n",
    "\n",
    "    def apply_recalibration_standards(self, recalibration_df):\n",
    "        # treatment_standard is a dictionary of outcome types and effect sizees\n",
    "        if self._bpTreatmentStrategy is not None:\n",
    "            if self._bpTreatmentStrategy.get_treatment_recalibration_for_population() is not None:\n",
    "                recalibration_df = self.recalibrate_bp_treatment(recalibration_df)\n",
    "        return recalibration_df\n",
    "\n",
    "    # should the estiamted treatment effect be based on the number of events in the population\n",
    "    # (i.e. # events treated / # of events untreated)\n",
    "    # of should it be based on teh predicted reisks\n",
    "    # the problem with the first approach is that its going to depend a lot on small sample sizes...\n",
    "    # and we don't necessarily want to take out that random error...that random error reflects\n",
    "    # genuine uncertainty.\n",
    "    # so, i thikn it should be based on the model-predicted risks...\n",
    "\n",
    "    def recalibrate_bp_treatment(self, recalibration_df):\n",
    "        #logging.info(f\"*** before recalibration, mi count: {recalibration_df.miNext.sum()}, stroke count: {recalibration_df.strokeNext.sum()}\")\n",
    "        treatment_outcome_standard = (\n",
    "            self._bpTreatmentStrategy.get_treatment_recalibration_for_population()\n",
    "        )\n",
    "        # estimate risk for the people alive at the start of the wave\n",
    "        recalibration_df= self.estimate_risks(recalibration_df, \"treated\")\n",
    "\n",
    "        # rollback the treatment effect.\n",
    "        # redtag: would like to apply to this to a deeply cloned population, but i can't get that to work\n",
    "        # so, for now, applying it to the actual population and then rolling the effect back later.\n",
    "        recalibration_df = recalibration_df.apply(\n",
    "            self._bpTreatmentStrategy.rollback_changes_vectorized, axis=\"columns\"\n",
    "        )\n",
    "\n",
    "        # estimate risk after applying the treamtent effect\n",
    "        recalibration_df = self.estimate_risks(recalibration_df, \"untreated\")\n",
    "\n",
    "        # hacktag related to above — roll back the treatment effect...\n",
    "        recalibration_df = recalibration_df.apply(\n",
    "            self._bpTreatmentStrategy.get_changes_vectorized, axis=\"columns\"\n",
    "        )\n",
    "        #logging.info(f\"######## BP meds After redo: {recalibration_df.totalBPMedsAddedNext.value_counts()}\")\n",
    "        totalBPMedsAddedCapped = recalibration_df['totalBPMedsAddedNext']\n",
    "        totalBPMedsAddedCapped.loc[totalBPMedsAddedCapped >= BaseTreatmentStrategy.MAX_BP_MEDS] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
    "        #recalibration_df.loc[recalibration_df['totalBPMedsAddedNext'] >= BaseTreatmentStrategy.MAX_BP_MEDS, 'totalBPMedsAddedCapped'] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
    "        recalibrationVars = {\"rolledBackEventType\" : [None] * len(recalibration_df),\n",
    "                            'totalBPMedsAddedCapped' : totalBPMedsAddedCapped}       \n",
    "        recalibration_df = pd.concat([recalibration_df.reset_index(drop=True), pd.DataFrame(recalibrationVars).reset_index(drop=True)], axis='columns', ignore_index=False)\n",
    "        \n",
    "        #recalibration_df[\"rolledBackEventType\"] = None\n",
    "        # total meds added represents the total number of medication effects that we'll recalibrate for\n",
    "        # it is the lesser of the total number of BP meds actually added (totalBpMedsAdded) or the max cap\n",
    "        # so, if a treamtent strategy adds 10 medications, they'll effect the BP...but, they \n",
    "        # wont' have an additional efect on event reduction over the medication cap\n",
    "        #logging.info(f\"######## BP meds After redo: {recalibration_df.totalBPMedsAddedNext.value_counts()}\")\n",
    "\n",
    "        # recalibrate within each group of added medicaitons so that we can stratify the treamtnet effects\n",
    "        for i in range(1, BaseTreatmentStrategy.MAX_BP_MEDS + 1):\n",
    "            #logging.info(f\"Roll back for med count: {i}\")\n",
    "            recalibrationPopForMedCount = recalibration_df.loc[recalibration_df.totalBPMedsAddedCapped == i]\n",
    "            # the change standards are for a single medication\n",
    "            recalibration_standard_for_med_count = treatment_outcome_standard.copy()\n",
    "            for key, value in recalibration_standard_for_med_count.items():\n",
    "                recalibration_standard_for_med_count[key] = value**i\n",
    "\n",
    "            if len(recalibrationPopForMedCount) > 0:\n",
    "                # recalibrate stroke\n",
    "                recalibratedForMedCount = self.create_or_rollback_events_to_correct_calibration(\n",
    "                    recalibration_standard_for_med_count,\n",
    "                    \"treatedstrokeRisks\",\n",
    "                    \"untreatedstrokeRisks\",\n",
    "                    \"stroke\",\n",
    "                    OutcomeType.STROKE,\n",
    "                    CVOutcomeDetermination()._will_have_fatal_stroke,\n",
    "                    recalibrationPopForMedCount,\n",
    "                )\n",
    "\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"strokeNext\"\n",
    "                ] = recalibratedForMedCount[\"strokeNext\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"strokeFatal\"\n",
    "                ] = recalibratedForMedCount[\"strokeFatal\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"deadNext\"\n",
    "                ] = recalibratedForMedCount[\"deadNext\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"ageAtFirstStroke\"\n",
    "                ] = recalibratedForMedCount[\"ageAtFirstStroke\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"rolledBackEventType\"\n",
    "                ] = recalibratedForMedCount[\"rolledBackEventType\"]\n",
    "\n",
    "                # recalibrate MI\n",
    "                recalibratedForMedCount = self.create_or_rollback_events_to_correct_calibration(\n",
    "                    recalibration_standard_for_med_count,\n",
    "                    \"treatedmiRisks\",\n",
    "                    \"untreatedmiRisks\",\n",
    "                    \"mi\",\n",
    "                    OutcomeType.MI,\n",
    "                    CVOutcomeDetermination()._will_have_fatal_mi,\n",
    "                    recalibrationPopForMedCount,\n",
    "                )\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"miNext\"\n",
    "                ] = recalibratedForMedCount[\"miNext\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"miFatal\"\n",
    "                ] = recalibratedForMedCount[\"miFatal\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"deadNext\"\n",
    "                ] = recalibratedForMedCount[\"deadNext\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"ageAtFirstMI\"\n",
    "                ] = recalibratedForMedCount[\"ageAtFirstMI\"]\n",
    "                recalibration_df.loc[\n",
    "                    recalibratedForMedCount.index, \"rolledBackEventType\"\n",
    "                ] = recalibratedForMedCount[\"rolledBackEventType\"]\n",
    "\n",
    "        #logging.info(f\"*** after recalibration, mi count: {recalibration_df.miNext.sum()}, stroke count: {recalibration_df.strokeNext.sum()}\")\n",
    "        recalibration_df.drop(columns=['treatedcombinedRisks', 'treatedstrokeProbabilities', 'treatedstrokeRisks', 'treatedmiRisks', \n",
    "                    'untreatedcombinedRisks', 'untreatedstrokeProbabilities', 'untreatedstrokeRisks', 'untreatedmiRisks', 'totalBPMedsAddedCapped', 'rolledBackEventType'], inplace=True)\n",
    "        return recalibration_df\n",
    "\n",
    "    def estimate_risks(self, recalibration_df, prefix):\n",
    "        combinedRisks = recalibration_df.apply(\n",
    "            self._outcome_model_repository.get_risk_for_person_vectorized,\n",
    "            axis=\"columns\",\n",
    "            args=(OutcomeModelType.CARDIOVASCULAR, 1),\n",
    "        )\n",
    "        strokeProbabilities = recalibration_df.apply(\n",
    "            CVOutcomeDetermination().get_stroke_probability, axis=\"columns\", vectorized=True\n",
    "        )\n",
    "        strokeRisks = (\n",
    "            combinedRisks * strokeProbabilities\n",
    "        )\n",
    "        miRisks = combinedRisks * (\n",
    "            1 - strokeProbabilities\n",
    "        )\n",
    "\n",
    "        risksAndProbs = pd.DataFrame({prefix + \"combinedRisks\" : combinedRisks, prefix + \"strokeProbabilities\" : strokeProbabilities,\n",
    "                        prefix + \"strokeRisks\" : strokeRisks, prefix + \"miRisks\" : miRisks})\n",
    "        \n",
    "        return pd.concat([recalibration_df.reset_index(drop=True), risksAndProbs.reset_index(drop=True)], axis='columns', ignore_index=False)\n",
    "\n",
    "\n",
    "    def create_or_rollback_events_to_correct_calibration(\n",
    "        self,\n",
    "        treatment_outcome_standard,\n",
    "        treatedRiskVar,\n",
    "        untreatedRiskVar,\n",
    "        eventVar,\n",
    "        outcomeType,\n",
    "        fatalityDetermination,\n",
    "        recalibration_pop,\n",
    "    ):\n",
    "        #logging.info(f\"create or rollback {outcomeType}, standard: {treatment_outcome_standard[outcomeType]}\")\n",
    "\n",
    "        modelEstimatedRR = (\n",
    "            recalibration_pop[treatedRiskVar].mean() / recalibration_pop[untreatedRiskVar].mean()\n",
    "        )\n",
    "        nextEventVar = eventVar + \"Next\"\n",
    "        ageAtFirstVar = (\n",
    "            \"ageAtFirst\" + eventVar.upper()\n",
    "            if len(eventVar) == 2\n",
    "            else \"ageAtFirst\" + eventVar.capitalize()\n",
    "        )\n",
    "        # use the delta between that effect and the calibration standard to recalibrate the pop.\n",
    "        delta = modelEstimatedRR - treatment_outcome_standard[outcomeType]\n",
    "        eventsForPeople = recalibration_pop.loc[recalibration_pop[nextEventVar] == True]\n",
    "\n",
    "        numberOfEventStatusesToChange = abs(\n",
    "            int(round(delta * len(eventsForPeople) / modelEstimatedRR))\n",
    "        )\n",
    "        nonEventsForPeople = recalibration_pop.loc[recalibration_pop[nextEventVar] == False]\n",
    "        # key assumption: \"treatment\" is applied to a population as opposed to individuals within a population\n",
    "        # analyses can be setup either way...build two populations and then set different treatments\n",
    "        # or build a ur-population adn then set different treamtents within them\n",
    "        # this is, i thikn, the first time where a coding decision is tied to one of those structure.\n",
    "        # it would not, i think, be hard to change. but, just spelling it out here.\n",
    "\n",
    "        # if negative, the model estimated too few events, if positive, too mnany\n",
    "        #logging.info(f\"bp recalibration, delta: {delta}, number of statuses to change: {numberOfEventStatusesToChange}\")\n",
    "\n",
    "        if delta < 0:\n",
    "            if numberOfEventStatusesToChange > 0:\n",
    "                new_events = nonEventsForPeople.sample(\n",
    "                    n=numberOfEventStatusesToChange,\n",
    "                    replace=False,\n",
    "                    weights=nonEventsForPeople[untreatedRiskVar].values,\n",
    "                )\n",
    "                recalibration_pop.loc[new_events.index, nextEventVar] = True\n",
    "                recalibration_pop.loc[\n",
    "                    new_events.index, eventVar + \"Fatal\"\n",
    "                ] = recalibration_pop.loc[new_events.index].apply(\n",
    "                    fatalityDetermination, axis=\"columns\", args=(True,)\n",
    "                )\n",
    "                recalibration_pop.loc[new_events.index, ageAtFirstVar] = np.fmin(\n",
    "                    recalibration_pop.loc[new_events.index].age,\n",
    "                    recalibration_pop.loc[new_events.index][ageAtFirstVar],\n",
    "                )\n",
    "\n",
    "        elif delta > 0:\n",
    "            if numberOfEventStatusesToChange > len(eventsForPeople):\n",
    "                numberOfEventStatusesToChange = len(eventsForPeople)\n",
    "            if numberOfEventStatusesToChange > 0:\n",
    "                events_to_rollback = eventsForPeople.sample(\n",
    "                    n=numberOfEventStatusesToChange,\n",
    "                    replace=False,\n",
    "                    weights=1 - eventsForPeople[untreatedRiskVar].values,\n",
    "                )\n",
    "                recalibration_pop.loc[events_to_rollback.index, nextEventVar] = False\n",
    "                recalibration_pop.loc[events_to_rollback.index, eventVar + \"Fatal\"] = False\n",
    "                recalibration_pop.loc[events_to_rollback.index, \"deadNext\"] = False\n",
    "                recalibration_pop.loc[events_to_rollback.index, ageAtFirstVar] = np.minimum(\n",
    "                    recalibration_pop.loc[events_to_rollback.index].age,\n",
    "                    recalibration_pop.loc[events_to_rollback.index][ageAtFirstVar],\n",
    "                )\n",
    "                recalibration_pop.loc[events_to_rollback.index, \"rolledBackEventType\"] = eventVar\n",
    "        return recalibration_pop\n",
    "\n",
    "    def get_people_alive_at_the_start_of_the_current_wave(self):\n",
    "        return self.get_people_alive_at_the_start_of_wave(self._currentWave)\n",
    "\n",
    "    def get_people_alive_at_the_start_of_wave(self, wave):\n",
    "        peopleAlive = []\n",
    "        for person in self._people:\n",
    "            if person.alive_at_start_of_wave(wave):\n",
    "                peopleAlive.append(person)\n",
    "        return pd.Series(peopleAlive)\n",
    "\n",
    "    def get_people_that_are_currently_alive(self):\n",
    "        return pd.Series([not person.is_dead() for _, person in self._people.items()])\n",
    "\n",
    "    def get_number_of_patients_currently_alive(self):\n",
    "        self.get_people_that_are_currently_alive().sum()\n",
    "\n",
    "    def get_events_in_most_recent_wave(self, eventType):\n",
    "        peopleWithEvents = []\n",
    "        for _, person in self._people.items():\n",
    "            if person.has_outcome_at_age(eventType, person._age[-1]):\n",
    "                peopleWithEvents.append(person)\n",
    "        return peopleWithEvents\n",
    "\n",
    "    def generate_starting_mean_patient(self):\n",
    "        df = self.get_people_initial_state_as_dataframe()\n",
    "        return Person(\n",
    "            age=int(round(df.age.mean())),\n",
    "            gender=NHANESGender(df.gender.mode()),\n",
    "            raceEthnicity=NHANESRaceEthnicity(df.raceEthnicity.mode()),\n",
    "            sbp=df.sbp.mean(),\n",
    "            dbp=df.dbp.mean(),\n",
    "            a1c=df.a1c.mean(),\n",
    "            hdl=df.hdl.mean(),\n",
    "            totChol=df.totChol.mean(),\n",
    "            bmi=df.bmi.mean(),\n",
    "            ldl=df.ldl.mean(),\n",
    "            trig=df.trig.mean(),\n",
    "            waist=df.waist.mean(),\n",
    "            anyPhysicalActivity=df.anyPhysicalActivity.mode(),\n",
    "            education=Education(df.education.mode()),\n",
    "            smokingStatus=SmokingStatus(df.smokingStatus.mode()),\n",
    "            antiHypertensiveCount=int(round(df.antiHypetensiveCount().mean())),\n",
    "            statin=df.statin.mode(),\n",
    "            otherLipidLoweringMedicationCount=int(\n",
    "                round(df.otherLipidLoweringMedicationCount.mean())\n",
    "            ),\n",
    "            initializeAfib=(lambda _: False),\n",
    "            selfReportStrokeAge=None,\n",
    "            selfReportMIAge=None,\n",
    "            randomEffects=self._outcome_model_repository.get_random_effects(),\n",
    "        )\n",
    "\n",
    "    def get_event_rate_in_simulation(self, eventType, duration):\n",
    "        events = [\n",
    "            person.has_outcome_during_simulation_prior_to_wave(eventType, duration)\n",
    "            for i, person in self._people.items()\n",
    "        ]\n",
    "        totalTime = [\n",
    "            person.years_in_simulation() if person.years_in_simulation() < duration else duration\n",
    "            for i, person in self._people.items()\n",
    "        ]\n",
    "        return np.array(events).sum() / np.array(totalTime).sum()\n",
    "\n",
    "    def get_raw_incidence_by_age(self, eventType):\n",
    "        popDF = self.get_people_current_state_as_dataframe()\n",
    "\n",
    "        for year in range(1, self._totalWavesAdvanced + 1):\n",
    "            eventVarName = \"event\" + str(year)\n",
    "            ageVarName = \"age\" + str(year)\n",
    "            popDF[ageVarName] = popDF[\"baseAge\"] + year\n",
    "            popDF[eventVarName] = [\n",
    "                person.has_outcome_during_wave(year, OutcomeType.DEMENTIA)\n",
    "                for person in self._people\n",
    "            ]\n",
    "\n",
    "        popDF = popDF[\n",
    "            list(filter(lambda x: x.startswith(\"age\") or x.startswith(\"event\"), popDF.columns))\n",
    "        ]\n",
    "        popDF[\"id\"] = popDF.index\n",
    "        popDF.drop(columns=[\"age\"], inplace=True)\n",
    "        longAgesEvents = pd.wide_to_long(df=popDF, stubnames=[\"age\", \"event\"], i=\"id\", j=\"wave\")\n",
    "\n",
    "        agesAliveDF = self.get_people_current_state_as_dataframe()\n",
    "        for year in range(1, self._totalWavesAdvanced + 1):\n",
    "            aliveVarName = \"alive\" + str(year)\n",
    "            ageVarName = \"age\" + str(year)\n",
    "            agesAliveDF[ageVarName] = agesAliveDF[\"baseAge\"] + year\n",
    "            agesAliveDF[aliveVarName] = [\n",
    "                person.alive_at_start_of_wave(year) for i, person in self._people.items()\n",
    "            ]\n",
    "\n",
    "        agesAliveDF = agesAliveDF[\n",
    "            list(\n",
    "                filter(lambda x: x.startswith(\"age\") or x.startswith(\"alive\"), agesAliveDF.columns)\n",
    "            )\n",
    "        ]\n",
    "        agesAliveDF.drop(columns=[\"age\"], inplace=True)\n",
    "        agesAliveDF[\"id\"] = agesAliveDF.index\n",
    "        longAgesDead = pd.wide_to_long(\n",
    "            df=agesAliveDF, stubnames=[\"age\", \"alive\"], i=\"id\", j=\"wave\"\n",
    "        )\n",
    "        return (\n",
    "            longAgesEvents.groupby(\"age\")[\"event\"].sum()\n",
    "            / longAgesDead.groupby(\"age\")[\"alive\"].sum()\n",
    "        )\n",
    "\n",
    "    # refactorrtag: we should probably build a specific class that loads data files...\n",
    "\n",
    "    def build_age_standard(self, yearOfStandardizedPopulation):\n",
    "        if yearOfStandardizedPopulation in Population._ageStandards:\n",
    "            return copy.deepcopy(Population._ageStandards[yearOfStandardizedPopulation])\n",
    "\n",
    "        datafile_path = get_absolute_datafile_path(\"us.1969_2017.19ages.adjusted.txt\")\n",
    "        ageStandard = pd.read_csv(datafile_path, header=0, names=[\"raw\"])\n",
    "        # https://seer.cancer.gov/popdata/popdic.html\n",
    "        ageStandard[\"year\"] = ageStandard[\"raw\"].str[0:4]\n",
    "        ageStandard[\"year\"] = ageStandard.year.astype(int)\n",
    "        # format changes in 1990...so, we'll go forward from there...\n",
    "        ageStandard = ageStandard.loc[ageStandard.year >= 1990]\n",
    "        ageStandard[\"state\"] = ageStandard[\"raw\"].str[4:6]\n",
    "        ageStandard[\"state\"] = ageStandard[\"raw\"].str[4:6]\n",
    "        # 1 = white, 2 = black, 3 = american indian/alaskan, 4 = asian/pacific islander\n",
    "        ageStandard[\"race\"] = ageStandard[\"raw\"].str[13:14]\n",
    "        ageStandard[\"hispanic\"] = ageStandard[\"raw\"].str[14:15]\n",
    "        ageStandard[\"female\"] = ageStandard[\"raw\"].str[15:16]\n",
    "        ageStandard[\"female\"] = ageStandard[\"female\"].astype(int)\n",
    "        ageStandard[\"female\"] = ageStandard[\"female\"].replace({1: 0, 2: 1})\n",
    "        ageStandard[\"ageGroup\"] = ageStandard[\"raw\"].str[16:18]\n",
    "        ageStandard[\"ageGroup\"] = ageStandard[\"ageGroup\"].astype(int)\n",
    "        ageStandard[\"standardPopulation\"] = ageStandard[\"raw\"].str[18:26]\n",
    "        ageStandard[\"standardPopulation\"] = ageStandard[\"standardPopulation\"].astype(int)\n",
    "        ageStandard[\"lowerAgeBound\"] = (ageStandard.ageGroup - 1) * 5\n",
    "        ageStandard[\"upperAgeBound\"] = (ageStandard.ageGroup * 5) - 1\n",
    "        ageStandard[\"lowerAgeBound\"] = ageStandard[\"lowerAgeBound\"].replace({-5: 0, 0: 1})\n",
    "        ageStandard[\"upperAgeBound\"] = ageStandard[\"upperAgeBound\"].replace({-1: 0, 89: 150})\n",
    "        ageStandardYear = ageStandard.loc[ageStandard.year == yearOfStandardizedPopulation]\n",
    "        ageStandardGroupby = ageStandardYear[\n",
    "            [\"female\", \"standardPopulation\", \"lowerAgeBound\", \"upperAgeBound\", \"ageGroup\"]\n",
    "        ].groupby([\"ageGroup\", \"female\"])\n",
    "        ageStandardHeaders = ageStandardGroupby.first()[[\"lowerAgeBound\", \"upperAgeBound\"]]\n",
    "        ageStandardHeaders[\"female\"] = ageStandardHeaders.index.get_level_values(1)\n",
    "        ageStandardPopulation = ageStandardYear[[\"female\", \"standardPopulation\", \"ageGroup\"]]\n",
    "        ageStandardPopulation = ageStandardPopulation.groupby([\"ageGroup\", \"female\"]).sum()\n",
    "        ageStandardPopulation = ageStandardHeaders.join(ageStandardPopulation, how=\"inner\")\n",
    "        # cache the age standard populations...they're not that big and it takes a while\n",
    "        # to build one\n",
    "        ageStandardPopulation[\"outcomeCount\"] = 0\n",
    "        ageStandardPopulation[\"simPersonYears\"] = 0\n",
    "        ageStandardPopulation[\"simPeople\"] = 0\n",
    "        Population._ageStandards[yearOfStandardizedPopulation] = copy.deepcopy(\n",
    "            ageStandardPopulation\n",
    "        )\n",
    "\n",
    "        return ageStandardPopulation\n",
    "\n",
    "    def tabulate_age_specific_rates(self, ageStandard):\n",
    "        ageStandard[\"percentStandardPopInGroup\"] = ageStandard[\"standardPopulation\"] / (\n",
    "            ageStandard[\"standardPopulation\"].sum()\n",
    "        )\n",
    "        ageStandard[\"ageSpecificRate\"] = (\n",
    "            ageStandard[\"outcomeCount\"] * 100000 / ageStandard[\"simPersonYears\"]\n",
    "        )\n",
    "        ageStandard[\"ageSpecificContribution\"] = (\n",
    "            ageStandard[\"ageSpecificRate\"] * ageStandard[\"percentStandardPopInGroup\"]\n",
    "        )\n",
    "        return ageStandard\n",
    "\n",
    "    # return the age standardized # of events per 100,000 person years\n",
    "    def calculate_mean_age_sex_standardized_incidence(\n",
    "        self,\n",
    "        outcomeType,\n",
    "        yearOfStandardizedPopulation=2016,\n",
    "        subPopulationSelector=None,\n",
    "        subPopulationDFSelector=None,\n",
    "    ):\n",
    "\n",
    "        # the age selector picks the first outcome (_outcomes(outcomeTYpe)[0]) and the age is the\n",
    "        # first element within the returned tuple (the second [0])\n",
    "        events = self.calculate_mean_age_sex_standardized_event(\n",
    "            lambda x: x.has_outcome_during_simulation(outcomeType),\n",
    "            lambda x: x.get_outcomes_during_simulation(outcomeType)[0][0] - x._age[0] + 1,\n",
    "            yearOfStandardizedPopulation,\n",
    "            subPopulationSelector,\n",
    "            subPopulationDFSelector,\n",
    "        )\n",
    "        return (\n",
    "            pd.Series([event[0] for event in events]).mean(),\n",
    "            pd.Series([event[1] for event in events]).sum(),\n",
    "        )\n",
    "\n",
    "    def calculate_mean_age_sex_standardized_mortality(self, yearOfStandardizedPopulation=2016):\n",
    "        events = self.calculate_mean_age_sex_standardized_event(\n",
    "            lambda x: x.is_dead(), lambda x: x.years_in_simulation(), yearOfStandardizedPopulation\n",
    "        )\n",
    "        return pd.Series([event[0] for event in events]).mean()\n",
    "\n",
    "    def get_events_for_event_type(\n",
    "        self,\n",
    "        eventSelector,\n",
    "        eventAgeIdentifier,\n",
    "        subPopulationSelector=None,\n",
    "        subPopulationDFSelector=None,\n",
    "    ):\n",
    "        # build a dataframe to represent the population\n",
    "        popDF = self.get_people_current_state_as_dataframe(parallel=False)\n",
    "        popDF[\"female\"] = popDF[\"gender\"] - 1\n",
    "\n",
    "        # calculated standardized event rate for each year\n",
    "        for year in range(1, self._totalWavesAdvanced + 1):\n",
    "            eventVarName = \"event\" + str(year)\n",
    "            ageVarName = \"age\" + str(year)\n",
    "            popDF[ageVarName] = popDF[\"baseAge\"] + year\n",
    "            if subPopulationDFSelector is not None:\n",
    "                popDF[\"subpopFilter\"] = popDF.apply(subPopulationDFSelector, axis=\"columns\")\n",
    "                popDF = popDF.loc[popDF.subpopFilter == 1]\n",
    "            popDF[eventVarName] = [\n",
    "                eventSelector(person) and eventAgeIdentifier(person) == year\n",
    "                for person in filter(subPopulationSelector, self._people)\n",
    "            ]\n",
    "        return popDF\n",
    "\n",
    "    def calculate_mean_age_sex_standardized_event(\n",
    "        self,\n",
    "        eventSelector,\n",
    "        eventAgeIdentifier,\n",
    "        yearOfStandardizedPopulation=2016,\n",
    "        subPopulationSelector=None,\n",
    "        subPopulationDFSelector=None,\n",
    "    ):\n",
    "        # calculated standardized event rate for each year\n",
    "        popDF = self.get_events_for_event_type(\n",
    "            eventSelector, eventAgeIdentifier, subPopulationSelector, subPopulationDFSelector\n",
    "        )\n",
    "        popDF[\"female\"] = popDF[\"gender\"] - 1\n",
    "\n",
    "        eventsPerYear = []\n",
    "\n",
    "        for year in range(1, self._totalWavesAdvanced + 1):\n",
    "            eventVarName = \"event\" + str(year)\n",
    "            ageVarName = \"age\" + str(year)\n",
    "            popDF[ageVarName] = popDF[\"baseAge\"] + year\n",
    "            if subPopulationDFSelector is not None:\n",
    "                popDF[\"subpopFilter\"] = popDF.apply(subPopulationDFSelector, axis=\"columns\")\n",
    "                popDF = popDF.loc[popDF.subpopFilter == 1]\n",
    "            popDF[eventVarName] = [\n",
    "                eventSelector(person) and eventAgeIdentifier(person) == year\n",
    "                for person in filter(subPopulationSelector, self._people)\n",
    "            ]\n",
    "            dfForAnnualEventCalc = popDF[[ageVarName, \"female\", eventVarName]]\n",
    "            dfForAnnualEventCalc.rename(\n",
    "                columns={ageVarName: \"age\", eventVarName: \"event\"}, inplace=True\n",
    "            )\n",
    "            eventsPerYear.append(\n",
    "                self.get_standardized_events_for_year(\n",
    "                    dfForAnnualEventCalc, yearOfStandardizedPopulation\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return eventsPerYear\n",
    "\n",
    "    def get_standardized_events_for_year(self, peopleDF, yearOfStandardizedPopulation):\n",
    "        ageStandard = self.build_age_standard(yearOfStandardizedPopulation)\n",
    "        # limit to the years where there are people\n",
    "        # if the simulation runs for 50 years...there will be empty cells in all of the\n",
    "        # young person categories\n",
    "        ageStandard = ageStandard.loc[ageStandard.lowerAgeBound >= peopleDF.age.min()]\n",
    "\n",
    "        # take the dataframe of peoplein teh population and tabnulate events relative\n",
    "        # to the age standard (max age is 85 in the age standard...)\n",
    "        peopleDF.loc[peopleDF[\"age\"] > 85, \"age\"] = 85\n",
    "        peopleDF.loc[:, \"ageGroup\"] = (peopleDF[\"age\"] // 5) + 1\n",
    "        peopleDF.loc[:, \"ageGroup\"] = peopleDF[\"ageGroup\"].astype(int)\n",
    "        # tabulate events by group\n",
    "        eventsByGroup = peopleDF.groupby([\"ageGroup\", \"female\"])[\"event\"].sum()\n",
    "        personYears = peopleDF.groupby([\"ageGroup\", \"female\"])[\"age\"].count()\n",
    "        # set those events on the age standard\n",
    "        ageStandard[\"outcomeCount\"] = eventsByGroup\n",
    "        ageStandard[\"simPersonYears\"] = personYears\n",
    "\n",
    "        ageStandard = self.tabulate_age_specific_rates(ageStandard)\n",
    "        return (ageStandard.ageSpecificContribution.sum(), ageStandard.outcomeCount.sum())\n",
    "\n",
    "    def get_person_attributes_from_person(self, person, timeVaryingCovariates):\n",
    "        attrForPerson = person.get_current_state_as_dict()\n",
    "        try:\n",
    "            attrForPerson[\"populationIndex\"] = person._populationIndex\n",
    "        except AttributeError:\n",
    "            pass  # populationIndex is not necessary for advancing; can continue safely without it\n",
    "\n",
    "        timeVaryingAttrsForPerson = person.get_tvc_state_as_dict(timeVaryingCovariates)\n",
    "        attrForPerson.update(timeVaryingAttrsForPerson)\n",
    "        return attrForPerson\n",
    "\n",
    "    #def get_people_current_state_as_dataframe(self, parallel=True): #CD modified\n",
    "    def get_people_current_state_as_dataframe(self, parallel=False):\n",
    "        if parallel:\n",
    "            pandarallel.initialize(verbose=1)\n",
    "            return pd.DataFrame(\n",
    "                list(\n",
    "                    self._people.parallel_apply(\n",
    "                        self.get_person_attributes_from_person,\n",
    "                        timeVaryingCovariates=self._timeVaryingCovariates,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            return pd.DataFrame(\n",
    "                list(\n",
    "                    self._people.apply(\n",
    "                        self.get_person_attributes_from_person,\n",
    "                        timeVaryingCovariates=self._timeVaryingCovariates,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def get_people_current_state_and_summary_as_dataframe(self):\n",
    "        df = self.get_people_current_state_as_dataframe()\n",
    "        # iterate through variables that vary over time\n",
    "        tvcMeans = {}\n",
    "        for var in self._timeVaryingCovariates:\n",
    "            tvcMeans[\"mean\" + var.capitalize()] = [\n",
    "                pd.Series(getattr(person, \"_\" + var)).mean()\n",
    "                for i, person in self._people.items()\n",
    "            ]   \n",
    "        return pd.concat([df, pd.DataFrame(tvcMeans)], axis=1)\n",
    "\n",
    "    def get_people_initial_state_as_dataframe(self):\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"age\": [person._age[0] for person in self._people],\n",
    "                \"gender\": [person._gender for person in self._people],\n",
    "                \"raceEthnicity\": [person._raceEthnicity for person in self._people],\n",
    "                \"sbp\": [person._sbp[0] for person in self._people],\n",
    "                \"dbp\": [person._dbp[0] for person in self._people],\n",
    "                \"a1c\": [person._a1c[0] for person in self._people],\n",
    "                \"hdl\": [person._hdl[0] for person in self._people],\n",
    "                \"ldl\": [person._ldl[0] for person in self._people],\n",
    "                \"trig\": [person._trig[0] for person in self._people],\n",
    "                \"totChol\": [person._totChol[0] for person in self._people],\n",
    "                \"creatinine\": [person._creatinine[0] for person in self._people],\n",
    "                \"bmi\": [person._bmi[0] for person in self._people],\n",
    "                \"anyPhysicalActivity\": [person._anyPhysicalActivity[0] for person in self._people],\n",
    "                \"education\": [person._education.value for person in self._people],\n",
    "                \"afib\": [person._afib[0] for person in self._people],\n",
    "                \"antiHypertensiveCount\": [\n",
    "                    person._antiHypertensiveCount[0] for person in self._people\n",
    "                ],\n",
    "                \"statin\": [person._statin[0] for person in self._people],\n",
    "                \"otherLipidLoweringMedicationCount\": [\n",
    "                    person._otherLipidLoweringMedicationCount[0] for person in self._people\n",
    "                ],\n",
    "                \"waist\": [person._waist[0] for person in self._people],\n",
    "                \"smokingStatus\": [person._smokingStatus for person in self._people],\n",
    "                \"miPriorToSim\": [person._selfReportMIPriorToSim for person in self._people],\n",
    "                \"strokePriorToSim\": [\n",
    "                    person._selfReportStrokePriorToSim for person in self._people\n",
    "                ],\n",
    "                \"totalQalys\": [np.array(person._qalys).sum() for person in self._people],\n",
    "                \"totalBPMedsAdded\" : [np.zeros(len(self._people))],\n",
    "                \"bpMedsAdded\" : [np.zeros(len(self._people))]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def get_summary_df(self):\n",
    "        data = {}\n",
    "        for year in range(1,self._currentWave+1):\n",
    "            data[f'mi{year}'] = [x.has_mi_during_wave(year) for _, x in self._people.items()]\n",
    "            data[f'stroke{year}'] = [x.has_stroke_during_wave(year) for _, x in self._people.items()]\n",
    "            data[f'dead{year}'] = [x.dead_at_end_of_wave(year) for _, x in self._people.items()]\n",
    "            data[f'dementia{year}'] = [x.has_outcome_during_wave(year, OutcomeType.DEMENTIA) for _, x in self._people.items()]\n",
    "            data[f'gcp{year}'] = [np.nan if x.dead_at_start_of_wave(year) else x._gcp[year-1] for _, x in self._people.items()]\n",
    "            data[f'sbp{year}'] = [np.nan if x.dead_at_start_of_wave(year) else x._sbp[year-1] for _, x in self._people.items()]\n",
    "            data[f'dbp{year}'] = [np.nan if x.dead_at_start_of_wave(year) else x._dbp[year-1] for _, x in self._people.items()]  \n",
    "            data[f'bpMeds{year}'] = [np.nan if x.dead_at_start_of_wave(year) else x._antiHypertensiveCount[year-1] for _, x in self._people.items()]\n",
    "            data[f'bpMedsAdded{year}'] = [np.nan if x.dead_at_start_of_wave(year) else x._bpMedsAdded[year-1] for _, x in self._people.items()]\n",
    "            data[f'totalBPMeds{year}'] = [np.nan if x.dead_at_start_of_wave(year) else x._bpMedsAdded[year-1]+x._antiHypertensiveCount[year-1] for _, x in self._people.items()]\n",
    "            data[f'totalBPMedsAdded{year}'] = [np.array(x._bpMedsAdded).sum() for _, x in self._people.items()]\n",
    "\n",
    "        data['baseAge'] = [x._age[0] for _, x in self._people.items()]\n",
    "        data['id'] = [x._populationIndex  for _, x in self._people.items()] \n",
    "        data['finalAge'] = [x._age[-1]  for _, x in self._people.items()]\n",
    "        data['education'] = [x._education  for _, x in self._people.items()]\n",
    "        data['gender'] = [x._gender  for _, x in self._people.items()]\n",
    "        data['raceEthnicity'] = [x._raceEthnicity  for _, x in self._people.items()]\n",
    "        data['smokingStatus'] = [x._smokingStatus  for _, x in self._people.items()]\n",
    "\n",
    "        data['baselineSBP'] = [x._sbp[0] for _, x in self._people.items()]\n",
    "        data['baselineDBP'] = [x._dbp[0] for _, x in self._people.items()]\n",
    "        data['black'] = [x._raceEthnicity==4 for _, x in self._people.items()]\n",
    "        data['dementiaFreeYears'] = [x.get_age_at_first_outcome(OutcomeType.DEMENTIA) - x._age[0] if x._dementia else x._age[-1] - x._age[0]  for _, x in self._people.items()]\n",
    "        data['deadAtEndOfSim'] = [x._alive[-1]==False for _, x in self._people.items()]\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "\n",
    "def initializeAFib(person):\n",
    "    model = load_regression_model(\"BaselineAFibModel\")\n",
    "    statsModel = StatsModelLogisticRiskFactorModel(model)\n",
    "    return statsModel.estimate_next_risk(person)\n",
    "\n",
    "\n",
    "def build_person(x, outcome_model_repository, randomEffects=None):\n",
    "    return Person(\n",
    "        age=x.age,\n",
    "        gender=NHANESGender(int(x.gender)),\n",
    "        raceEthnicity=NHANESRaceEthnicity(int(x.raceEthnicity)),\n",
    "        sbp=x.meanSBP,\n",
    "        dbp=x.meanDBP,\n",
    "        a1c=x.a1c,\n",
    "        hdl=x.hdl,\n",
    "        ldl=x.ldl,\n",
    "        trig=x.trig,\n",
    "        totChol=x.tot_chol,\n",
    "        bmi=x.bmi,\n",
    "        waist=x.waist,\n",
    "        anyPhysicalActivity=x.anyPhysicalActivity,\n",
    "        smokingStatus=SmokingStatus(int(x.smokingStatus)),\n",
    "        alcohol=AlcoholCategory.get_category_for_consumption(x.alcoholPerWeek),\n",
    "        education=Education(int(x.education)),\n",
    "        antiHypertensiveCount=x.antiHypertensive,\n",
    "        statin=x.statin,\n",
    "        otherLipidLoweringMedicationCount=x.otherLipidLowering,\n",
    "        creatinine=x.serumCreatinine,\n",
    "        initializeAfib=initializeAFib,\n",
    "        initializationRepository=InitializationRepository(),\n",
    "        selfReportStrokeAge=x.selfReportStrokeAge,\n",
    "        selfReportMIAge=np.random.randint(18, x.age)\n",
    "        if x.selfReportMIAge == 99999\n",
    "        else x.selfReportMIAge,\n",
    "        randomEffects=outcome_model_repository.get_random_effects() if randomEffects is None else randomEffects,\n",
    "        dfIndex=x.name,\n",
    "        diedBy2015=x.diedBy2015 == True,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_people_using_nhanes_for_sampling(\n",
    "    nhanes, n, outcome_model_repository, filter=None, random_seed=None, weights=None\n",
    "):\n",
    "    if weights is None:\n",
    "        weights = nhanes.WTINT2YR\n",
    "    repeated_sample = nhanes.sample(n, weights=weights, random_state=random_seed, replace=True)\n",
    "    pandarallel.initialize(verbose=1)\n",
    "    people = repeated_sample.parallel_apply(\n",
    "        build_person, outcome_model_repository=outcome_model_repository, axis=\"columns\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(people)):\n",
    "        people.iloc[i]._populationIndex = i\n",
    "\n",
    "    if filter is not None:\n",
    "        people = people.loc[people.apply(filter)]\n",
    "\n",
    "    return people\n",
    "\n",
    "class NHANESDirectSamplePopulation(Population):\n",
    "    \"\"\"Simple base class to sample with replacement from 2015/2016 NHANES\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n,\n",
    "        year,\n",
    "        filter=None,\n",
    "        generate_new_people=True,\n",
    "        model_reposistory_type=\"cohort\",\n",
    "        random_seed=None,\n",
    "        weights=None,\n",
    "    ):\n",
    "\n",
    "        nhanes = pd.read_stata(\"microsim/data/fullyImputedDataset.dta\")\n",
    "        nhanes = nhanes.loc[nhanes.year == year]\n",
    "        self._outcome_model_repository = OutcomeModelRepository()\n",
    "        people = build_people_using_nhanes_for_sampling(\n",
    "            nhanes,\n",
    "            n,\n",
    "            self._outcome_model_repository,\n",
    "            filter=filter,\n",
    "            random_seed=random_seed,\n",
    "            weights=weights,\n",
    "        )\n",
    "        super().__init__(people)\n",
    "        self._qaly_assignment_strategy = QALYAssignmentStrategy()\n",
    "        self.n = n\n",
    "        self.year = year\n",
    "        self._initialize_risk_models(model_reposistory_type)\n",
    "\n",
    "    def copy(self):\n",
    "        newPop = NHANESDirectSamplePopulation(self.n, self.year, False)\n",
    "        newPop._people = copy.deepcopy(self._people)\n",
    "        return newPop\n",
    "\n",
    "    def _initialize_risk_models(self, model_repository_type):\n",
    "        if model_repository_type == \"cohort\":\n",
    "            self._risk_model_repository = CohortRiskModelRepository()\n",
    "        elif model_repository_type == \"nhanes\":\n",
    "            self._risk_model_repository = NHANESRiskModelRepository()\n",
    "        else:\n",
    "            raise Exception(\"unknwon risk model repository type\" + model_repository_type)\n",
    "\n",
    "class PersonListPopulation(Population):\n",
    "    def __init__(self, people):\n",
    "\n",
    "        super().__init__(pd.Series(people))\n",
    "        self.n = len(people)\n",
    "        self._qaly_assignment_strategy = QALYAssignmentStrategy()\n",
    "        self._outcome_model_repository = OutcomeModelRepository()\n",
    "        self._risk_model_repository = CohortRiskModelRepository()\n",
    "        # population index is used for efficiency in the population, need to set it on \n",
    "        # each person when a new population is setup\n",
    "        for i, person in self._people.items():\n",
    "            person._populationIndex = i\n",
    "        # if the people have already been advanced, have the population start at that point\n",
    "        self._currentWave = len(people[0]._age)-1\n",
    "\n",
    "\n",
    "class NHANESAgeStandardPopulation(NHANESDirectSamplePopulation):\n",
    "    def __init__(self, n, year):\n",
    "        nhanes = pd.read_stata(\"microsim/data/fullyImputedDataset.dta\")\n",
    "        weights = self.get_weights(year)\n",
    "        weights[\"gender\"] = weights[\"female\"] + 1\n",
    "        weights = pd.merge(nhanes, weights, how=\"left\", on=[\"age\", \"gender\"]).popWeight\n",
    "        super().__init__(n=n, year=year, weights=weights)\n",
    "\n",
    "    def get_weights(self, year):\n",
    "        standard = self.build_age_standard(year)\n",
    "        return self.get_population_weighted_standard(standard)\n",
    "\n",
    "    def get_population_weighted_standard(self, standard):\n",
    "        rows = []\n",
    "        for age in range(1, 151):\n",
    "            for female in range(0, 2):\n",
    "                dfRow = standard.loc[\n",
    "                    (age >= standard.lowerAgeBound)\n",
    "                    & (age <= standard.upperAgeBound)\n",
    "                    & (standard.female == female)\n",
    "                ]\n",
    "                upperAge = dfRow[\"upperAgeBound\"].values[0]\n",
    "                lowerAge = dfRow[\"lowerAgeBound\"].values[0]\n",
    "                totalPop = dfRow[\"standardPopulation\"].values[0]\n",
    "                rows.append(\n",
    "                    {\"age\": age, \"female\": female, \"pop\": totalPop / (upperAge - lowerAge + 1)}\n",
    "                )\n",
    "        df = pd.DataFrame(rows)\n",
    "        df[\"popWeight\"] = df[\"pop\"] / df[\"pop\"].sum()\n",
    "        return df\n",
    "\n",
    "\n",
    "class ClonePopulation(Population):\n",
    "    \"\"\"Simple class to build a \"Population\" seeded by mulitple copies of the same person\"\"\"\n",
    "\n",
    "    def __init__(self, person, n):\n",
    "        self._outcome_model_repository = OutcomeModelRepository()\n",
    "        self._qaly_assignment_strategy = QALYAssignmentStrategy()\n",
    "        self._risk_model_repository = CohortRiskModelRepository()\n",
    "        self._initialization_repository = InitializationRepository()\n",
    "        self.n = n\n",
    "\n",
    "        # trying to make sure that cloned peopel are setup the same way as people are\n",
    "        # when sampled from NHANES\n",
    "        clonePerson = build_person(pd.Series({'age' : person._age[0],\n",
    "                                            'gender': int(person._gender),\n",
    "                                            'raceEthnicity':int(person._raceEthnicity),\n",
    "                                            'meanSBP' :person._sbp[0],\n",
    "                                            'meanDBP' :person._dbp[0],\n",
    "                                            'a1c':person._a1c[0],\n",
    "                                            'hdl':person._hdl[0],\n",
    "                                            'ldl':person._ldl[0],\n",
    "                                            'trig':person._trig[0],\n",
    "                                            'tot_chol':person._totChol[0],\n",
    "                                            'bmi':person._bmi[0],\n",
    "                                            'waist':person._waist[0],\n",
    "                                            'anyPhysicalActivity':person._anyPhysicalActivity[0],\n",
    "                                            'smokingStatus': int(person._smokingStatus),\n",
    "                                            'alcoholPerWeek': person._alcoholPerWeek[0],\n",
    "                                            'education' : int(person._education),\n",
    "                                            'antiHypertensive': person._antiHypertensiveCount[0],\n",
    "                                            'statin': person._statin[0],\n",
    "                                            'otherLipidLowering' : person._otherLipidLoweringMedicationCount[0],\n",
    "                                            'serumCreatinine' : person._creatinine[0],\n",
    "                                            'selfReportStrokeAge' : -1, \n",
    "                                            'selfReportMIAge' : -1,\n",
    "                                            'diedBy2015' : 0}), \n",
    "                                            self._outcome_model_repository, \n",
    "                                            randomEffects=person._randomEffects)\n",
    "\n",
    "        # for factors that were initialized on the first person, we have to set them the same way on teh clones\n",
    "        clonePerson._afib[0] = person._afib[0]\n",
    "        initializers = self._initialization_repository.get_initializers()\n",
    "        for initializerName, _ in initializers.items():\n",
    "            fromAttr = getattr(person, initializerName)\n",
    "            toAttr = getattr(clonePerson, initializerName)\n",
    "            toAttr.clear()\n",
    "            toAttr.extend(fromAttr)\n",
    "\n",
    "        \n",
    "        people = pd.Series([copy.deepcopy(clonePerson) for i in range(0, n)])\n",
    "\n",
    "        #pandarallel.initialize(verbose=1)\n",
    "        for i in range(0, len(people)):\n",
    "            people.iloc[i]._populationIndex = i\n",
    "        super().__init__(people)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c904515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying a small population for now to get things moving quickly\n",
    "pop = NHANESDirectSamplePopulation(10000, 1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e991df18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing year: 0\n"
     ]
    }
   ],
   "source": [
    "alive, df = pop.advance_vectorized(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6ada7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vascularEventOrDeath = LogisticRegressionAnalysis(OutcomeAssessor([OutcomeAssessor.DEATH, OutcomeType.STROKE, OutcomeType.MI]))\n",
    "anyEvent = LogisticRegressionAnalysis(OutcomeAssessor([OutcomeAssessor.DEATH, OutcomeType.STROKE, OutcomeType.MI, OutcomeType.DEMENTIA]))\n",
    "death = LogisticRegressionAnalysis(OutcomeAssessor([OutcomeAssessor.DEATH]))\n",
    "qalys = LinearRegressionAnalysis(AttributeOutcomeAssessor(\"_qalys\", AssessmentMethod.SUM))\n",
    "meanGCP = LinearRegressionAnalysis(AttributeOutcomeAssessor(\"_gcp\", AssessmentMethod.MEAN))\n",
    "lastGCP = LinearRegressionAnalysis(AttributeOutcomeAssessor(\"_gcp\", AssessmentMethod.LAST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df0d9cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code has access to cores  4\n"
     ]
    }
   ],
   "source": [
    "#we will later define how many processes to launch, this is just for validation\n",
    "#and so that we do not set the number of processes to a number greater than the number of available cores\n",
    "print(\"code has access to cores \",len(os.sched_getaffinity(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d723d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are usually set from the input file, but to make things easier here \n",
    "#just set them on the notebook (which means no script will test for their meaningfulness)\n",
    "inputSampleSizes = [500,1000] #for quick tests\n",
    "#inputSampleSizes = [100, 200, 500, 1000, 5000, 10000, 15000, 20000]\n",
    "inputDurations = [1,2] #for quick tests\n",
    "#inputDurations = [3,5,10,15,20]\n",
    "#inputDemThresholds = [2.4845839854531493e-08, 0.00018576417292080007, 0.0012917270937081809, 0.005870510161620921, 0.025739443157677927]\n",
    "#inputCvThresholds = [1.167603052003119e-06, 0.0008193743487641601, 0.0026191105926681, 0.006091251406939853, 0.0132184645579298]\n",
    "inputDemThresholds = [2.4845839854531493e-08] #for quick tests\n",
    "inputCvThresholds = [1.167603052003119e-06] #for quick tests\n",
    "inputTrialsetSize = 3\n",
    "inputProcesses = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe7258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perhaps these could go to trial_utils.py\n",
    "#since I need them in the trial descriptions \n",
    "\n",
    "#this function can be generalized, passing the kind of distribution as argument,if other distributions may be useful\n",
    "def randomizationSchemaUniform(x):\n",
    "    return (np.random.uniform() < 0.5)\n",
    "\n",
    "#riskDictionary will hold specific risk factor thresholds  \n",
    "#each trial will set its own cv and dem thresholds on this dictionary during initialization\n",
    "#also, should this function create the additional labels we include in results? \n",
    "#otherwise, is there a chance that the inclusion filter and labels be actually different?\n",
    "def inclusionFilter(person,riskDictionary): #added riskDictionary to arguments\n",
    "    #return RiskFilter({OutcomeModelType.DEMENTIA : dem , OutcomeModelType.CARDIOVASCULAR : cv}).exceedsThresholds(x)\n",
    "    return RiskFilter(riskDictionary).exceedsThresholds(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5efcd45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialDescription:\n",
    "    #right now all trials of a set share a trial description and differ in their risk thresholds\n",
    "    #as is, I cannot fully specify inclusionFilter (I mean I cannot provide arguments to inclusionFilter)\n",
    "    #if we decide to have each trial a different trialdescription, differing in their risk \n",
    "    #factors and/or risk thresholds then we could fully \n",
    "    #specify here the inclusionFilter and risk thresholds\n",
    "    def __init__(self, sampleSizes, durations, inclusionFilter, exclusionFilter, analyses, treatment,\n",
    "                #python pickles everything when it transfers things to the processes and \n",
    "                #lambda functions are not picklable, so I need to define a function for this\n",
    "                #randomizationSchema=lambda x : np.random.uniform() < 0.5): \n",
    "                randomizationSchema):\n",
    "        self.sampleSizes = sampleSizes\n",
    "        self.durations = durations\n",
    "        self.inclusionFilter = inclusionFilter\n",
    "        self.exclusionFilter = exclusionFilter\n",
    "        self.randomizationSchema = randomizationSchema\n",
    "        self.treatment = treatment\n",
    "        self.analyses = analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b39ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trial:\n",
    "    \n",
    "    #included risk dictionary and additional labels as part of trial instances\n",
    "    #riskDictionary is used during instance initialization in select_trial_population\n",
    "    #and not used after that, so I am not sure if it needs to be an instance attribute or not\n",
    "    #def __init__(self, trialDescription, targetPopulation):\n",
    "    def __init__(self, trialDescription, targetPopulation, riskDictionary, additionalLabels=None): \n",
    "        self.trialDescription = trialDescription\n",
    "        self.trialPopulation = self.select_trial_population(targetPopulation,\n",
    "            trialDescription.inclusionFilter, trialDescription.exclusionFilter,riskDictionary)\n",
    "        # select our patients from the population\n",
    "        self.maxSampleSize = pd.Series(trialDescription.sampleSizes).max()\n",
    "        self.treatedPop, self.untreatedPop = self.randomize(trialDescription.randomizationSchema)\n",
    "        self.analyticResults = {}\n",
    "        self.additionalLabels = additionalLabels #included labels as part of trial instances\n",
    "    #def select_trial_population(self, targetPopulation, inclusionFilter, exclusionFilter):\n",
    "    def select_trial_population(self, targetPopulation, inclusionFilter, exclusionFilter,riskDictionary):\n",
    "        #filter takes only one argument so I need to rewrite this without using filter\n",
    "        #so that I can pass both the person and the specific risk dictionary for this trial\n",
    "        #filteredPeople = list(filter(inclusionFilter, list(targetPopulation._people))) \n",
    "        filteredPeople = [person for person in list(targetPopulation._people) \n",
    "                          if inclusionFilter(person,riskDictionary)]\n",
    "        return PersonListPopulation(filteredPeople)\n",
    "    def randomize(self, randomizationSchema):\n",
    "        treatedList = []\n",
    "        untreatedList = []\n",
    "        randomizedCount = 0\n",
    "        # might be able to make this more efficient by sampling from the filtered people...\n",
    "        for i, person in self.trialPopulation._people.iteritems():\n",
    "            while randomizedCount < self.maxSampleSize:\n",
    "                if not person.is_dead():\n",
    "                    if randomizationSchema(person):\n",
    "                        treatedList.append(copy.deepcopy(person))\n",
    "                    else:\n",
    "                        untreatedList.append(copy.deepcopy(person))\n",
    "                    randomizedCount+=1\n",
    "                else:\n",
    "                    continue\n",
    "        return PersonListPopulation(treatedList), PersonListPopulation(untreatedList)\n",
    "    def run(self):\n",
    "        self.treatedPop._bpTreatmentStrategy = self.trialDescription.treatment\n",
    "        lastDuration = 0\n",
    "        for duration in self.trialDescription.durations:\n",
    "            self.treatedDF, self.treatedAlive = self.treatedPop.advance_vectorized(duration-lastDuration)\n",
    "            self.untreatedDF, self.untreatedAlive = self.untreatedPop.advance_vectorized(duration-lastDuration)\n",
    "            self.analyze(duration, \n",
    "                         self.maxSampleSize, \n",
    "                         self.treatedPop._people.tolist(), \n",
    "                         self.untreatedPop._people.tolist())\n",
    "            self.analyzeSmallerTrials(duration)\n",
    "            lastDuration = duration\n",
    "    def analyzeSmallerTrials(self, duration):\n",
    "        for sampleSize in self.trialDescription.sampleSizes:\n",
    "            numTrialsForSample = self.maxSampleSize // sampleSize\n",
    "            for i in range(0, numTrialsForSample):\n",
    "                if sampleSize==self.maxSampleSize:\n",
    "                    continue\n",
    "                sampleTreated = self.treatedPop._people.sample(int(sampleSize/2))\n",
    "                sampleUntreated = self.untreatedPop._people.sample(int(sampleSize/2))\n",
    "                self.analyze(duration, sampleSize, sampleTreated.tolist(), sampleUntreated.tolist())\n",
    "    def analyze(self, duration, sampleSize, treatedPopList, untreatedPopList):\n",
    "        for analysis in self.trialDescription.analyses:\n",
    "            reg, se, pvalue = None, None, None\n",
    "            try:\n",
    "                reg, se, pvalue = analysis.analyze(treatedPopList, untreatedPopList)\n",
    "            except PerfectSeparationError: # how to track these is not obvious, now now we'll enter \"Nones\"\n",
    "                pass\n",
    "            self.analyticResults[get_analysis_name(analysis, duration, sampleSize)] = \\\n",
    "                                                        {  'reg' : reg,\n",
    "                                                            'se' : se,\n",
    "                                                            'pvalue': pvalue,\n",
    "                                                            'duration' : duration,\n",
    "                                                            'sampleSize' : sampleSize,\n",
    "                                                            'outcome' :  analysis.outcomeAssessor.get_name(),\n",
    "                                                            'analysis' : analysis.name}\n",
    "        return self.analyticResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f0ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I tried at the beginning to bring all of these functions in the existing Trialset class but it seemed too \n",
    "#complex of a task to start with\n",
    "#so I started from scratch here another class\n",
    "#with your input, I can make changes and eventually move these to Trialset\n",
    "\n",
    "class TrialsetParallel:\n",
    "    \n",
    "    #demThresholds and cvThresholds are associated with riskDictionary and additionalLabels\n",
    "    #there is a potential of merging and simplifying these perhaps\n",
    "    def __init__(self, \n",
    "                 trialDescription, pop, trialCount, \n",
    "                 processesCount, \n",
    "                 demThresholds, cvThresholds, additionalLabels=None):\n",
    "        self.trialDescription = trialDescription\n",
    "        self.pop = pop\n",
    "        self.trialCount = trialCount #this is per set of risk factors, this is not the total!\n",
    "        self.processesCount = processesCount\n",
    "        self.demThresholds = demThresholds\n",
    "        self.cvThresholds = cvThresholds\n",
    "        self.additionalLabels = additionalLabels  \n",
    "    \n",
    "    #prepare all arguments needed to run the entire trial set \n",
    "    def prepareArgsForParallelRun(self):\n",
    "        argsForParallelRun = [] #arguments will be stored in a list of tuples (multiprocessing requirement)\n",
    "        for dem in self.demThresholds:\n",
    "            for cv in self.cvThresholds:\n",
    "                for iTrial in range(0,self.trialCount):\n",
    "                    #multiprocessing map functions accept only tuples to be sent to processes\n",
    "                    #the description and population are now available to the class methods as attributes\n",
    "                    #so the only thing I need to pass are the risk factors and thresholds\n",
    "                    #for as many trials as the set asks for\n",
    "                    argsForParallelRun.append(tuple((dem,cv)))\n",
    "        return argsForParallelRun\n",
    "    \n",
    "    #having the population as an argument passed to each trial is the reason why python makes copies of the population\n",
    "    #when it sends information to the cores running the processes with multiprocessing\n",
    "    #currently, this adds a RAM cost = (number of processes) * (population size in RAM)\n",
    "    #which suggests a solution: do not pass the actual population to the function the processes run but\n",
    "    #some kind of index/link/list/function as an interface to the population\n",
    "    def prepareRunAnalyzeTrial(self,dem,cv):\n",
    "        \n",
    "        print(\"starting a trial now\") #just to see if the calculation is indeed done in parallel\n",
    "        riskDictionary={OutcomeModelType.DEMENTIA : dem , OutcomeModelType.CARDIOVASCULAR : cv}\n",
    "        \n",
    "        trial = Trial(self.trialDescription, \n",
    "                      self.pop, \n",
    "                      riskDictionary,\n",
    "                      additionalLabels={'dementiaRisk' : dem, 'cvRisk' : cv}) #initialize trial\n",
    "        \n",
    "        trial.run() #run trial\n",
    "        \n",
    "        resultsForTrial = [] \n",
    "        for analysis in trial.trialDescription.analyses:\n",
    "            for duration in trial.trialDescription.durations:\n",
    "                for sampleSize in trial.trialDescription.sampleSizes:\n",
    "                    resultsForTrial.append(trial.analyticResults[get_analysis_name(analysis, duration, sampleSize)])\n",
    "        dfForTrial = pd.DataFrame(resultsForTrial)\n",
    "        if trial.additionalLabels is not None:\n",
    "            for label, labelVal in trial.additionalLabels.items():\n",
    "                dfForTrial[label] = labelVal\n",
    "                \n",
    "        #not sure if this is necessary in order to release the memory or not \n",
    "        #because python keeps track of references to objects anyway\n",
    "        del trial \n",
    "        print(\"ending a trial now\")\n",
    "        return dfForTrial #return only what I need: results\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        with mp.Pool(self.processesCount) as myPool: #context manager will terminate this pool\n",
    "            \n",
    "            #run trials and get back the list of dataframes with the results (no instance is returned)\n",
    "            resultsTrialsetList = myPool.starmap(self.prepareRunAnalyzeTrial, self.prepareArgsForParallelRun())\n",
    "            #convert list of dataframes to a single dataframe\n",
    "            resultsTrialsetPd = pd.concat(resultsTrialsetList).reset_index(drop=True)\n",
    "            \n",
    "        return resultsTrialsetPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdbb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this trial description will be the same for all trials in the trial set\n",
    "#the risk factor thresholds are set in method prepareRunAnalyzeTrial of TrialsetParallel\n",
    "desc = TrialDescription(sampleSizes= inputSampleSizes,\n",
    "                                                durations = inputDurations,\n",
    "                                                inclusionFilter=inclusionFilter,\n",
    "                                                exclusionFilter=None,\n",
    "                                                analyses=[death,\n",
    "                                                        anyEvent,\n",
    "                                                        vascularEventOrDeath,\n",
    "                                                        qalys,\n",
    "                                                        meanGCP,\n",
    "                                                        lastGCP],\n",
    "                                                treatment=SprintTreatment(),\n",
    "                                                randomizationSchema=randomizationSchemaUniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bac72060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the set\n",
    "trialset = TrialsetParallel(desc,pop,inputTrialsetSize,inputProcesses,inputDemThresholds,inputCvThresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9f75023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting a trial now\n",
      "starting a trial now\n",
      "starting a trial now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurmtmp.13936617/ipykernel_216860/2371660778.py:29: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for i, person in self.trialPopulation._people.iteritems():\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2371660778.py:29: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for i, person in self.trialPopulation._people.iteritems():\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2371660778.py:29: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for i, person in self.trialPopulation._people.iteritems():\n",
      "INFO:root:processing year: 1\n",
      "INFO:root:processing year: 1\n",
      "INFO:root:processing year: 1\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2300020484.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  totalBPMedsAddedCapped.loc[totalBPMedsAddedCapped >= BaseTreatmentStrategy.MAX_BP_MEDS] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2300020484.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  totalBPMedsAddedCapped.loc[totalBPMedsAddedCapped >= BaseTreatmentStrategy.MAX_BP_MEDS] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2300020484.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  totalBPMedsAddedCapped.loc[totalBPMedsAddedCapped >= BaseTreatmentStrategy.MAX_BP_MEDS] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
      "INFO:root:processing year: 1\n",
      "INFO:root:processing year: 1\n",
      "INFO:root:processing year: 1\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "INFO:root:processing year: 2\n",
      "INFO:root:processing year: 2\n",
      "INFO:root:processing year: 2\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2300020484.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  totalBPMedsAddedCapped.loc[totalBPMedsAddedCapped >= BaseTreatmentStrategy.MAX_BP_MEDS] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2300020484.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  totalBPMedsAddedCapped.loc[totalBPMedsAddedCapped >= BaseTreatmentStrategy.MAX_BP_MEDS] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
      "/tmp/slurmtmp.13936617/ipykernel_216860/2300020484.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  totalBPMedsAddedCapped.loc[totalBPMedsAddedCapped >= BaseTreatmentStrategy.MAX_BP_MEDS] = BaseTreatmentStrategy.MAX_BP_MEDS\n",
      "INFO:root:processing year: 2\n",
      "INFO:root:processing year: 2\n",
      "INFO:root:processing year: 2\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending a trial now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n",
      "/users/PAS2164/deligkaris/.cache/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/lib/python3.9/site-packages/statsmodels/base/optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method newton is: tol. The list of unsupported keyword arguments passed include: method_kwargs. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending a trial now\n",
      "ending a trial now\n"
     ]
    }
   ],
   "source": [
    "if __name__ ==  '__main__': #launching processes with multiprocesses requires this\n",
    "    results = trialset.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98b525b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg</th>\n",
       "      <th>se</th>\n",
       "      <th>pvalue</th>\n",
       "      <th>duration</th>\n",
       "      <th>sampleSize</th>\n",
       "      <th>outcome</th>\n",
       "      <th>analysis</th>\n",
       "      <th>dementiaRisk</th>\n",
       "      <th>cvRisk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.089571e-16</td>\n",
       "      <td>1.417051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>death</td>\n",
       "      <td>logisticRegression-death</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.016604e-02</td>\n",
       "      <td>1.002007</td>\n",
       "      <td>0.968025</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>death</td>\n",
       "      <td>logisticRegression-death</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>death</td>\n",
       "      <td>logisticRegression-death</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.476824e-01</td>\n",
       "      <td>0.915071</td>\n",
       "      <td>0.624677</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>death</td>\n",
       "      <td>logisticRegression-death</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.089571e-16</td>\n",
       "      <td>1.417051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>deathstroke-mi-dementia-</td>\n",
       "      <td>logisticRegression-deathstroke-mi-dementia-</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2.727091e-01</td>\n",
       "      <td>0.157073</td>\n",
       "      <td>0.082838</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>_gcp-mean</td>\n",
       "      <td>linearRegression-_gcp-mean</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.496000e+00</td>\n",
       "      <td>0.624071</td>\n",
       "      <td>0.016890</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>_gcp-last</td>\n",
       "      <td>linearRegression-_gcp-last</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.308123e-01</td>\n",
       "      <td>0.438990</td>\n",
       "      <td>0.226884</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>_gcp-last</td>\n",
       "      <td>linearRegression-_gcp-last</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.280000e-01</td>\n",
       "      <td>0.596910</td>\n",
       "      <td>0.376823</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>_gcp-last</td>\n",
       "      <td>linearRegression-_gcp-last</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4.291317e-01</td>\n",
       "      <td>0.427344</td>\n",
       "      <td>0.315534</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>_gcp-last</td>\n",
       "      <td>linearRegression-_gcp-last</td>\n",
       "      <td>2.484584e-08</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             reg        se    pvalue  duration  sampleSize  \\\n",
       "0   3.089571e-16  1.417051  1.000000         1         500   \n",
       "1  -4.016604e-02  1.002007  0.968025         1        1000   \n",
       "2            NaN       NaN       NaN         2         500   \n",
       "3  -4.476824e-01  0.915071  0.624677         2        1000   \n",
       "4   3.089571e-16  1.417051  1.000000         1         500   \n",
       "..           ...       ...       ...       ...         ...   \n",
       "67  2.727091e-01  0.157073  0.082838         2        1000   \n",
       "68  1.496000e+00  0.624071  0.016890         1         500   \n",
       "69  5.308123e-01  0.438990  0.226884         1        1000   \n",
       "70  5.280000e-01  0.596910  0.376823         2         500   \n",
       "71  4.291317e-01  0.427344  0.315534         2        1000   \n",
       "\n",
       "                     outcome                                     analysis  \\\n",
       "0                      death                     logisticRegression-death   \n",
       "1                      death                     logisticRegression-death   \n",
       "2                      death                     logisticRegression-death   \n",
       "3                      death                     logisticRegression-death   \n",
       "4   deathstroke-mi-dementia-  logisticRegression-deathstroke-mi-dementia-   \n",
       "..                       ...                                          ...   \n",
       "67                 _gcp-mean                   linearRegression-_gcp-mean   \n",
       "68                 _gcp-last                   linearRegression-_gcp-last   \n",
       "69                 _gcp-last                   linearRegression-_gcp-last   \n",
       "70                 _gcp-last                   linearRegression-_gcp-last   \n",
       "71                 _gcp-last                   linearRegression-_gcp-last   \n",
       "\n",
       "    dementiaRisk    cvRisk  \n",
       "0   2.484584e-08  0.000001  \n",
       "1   2.484584e-08  0.000001  \n",
       "2   2.484584e-08  0.000001  \n",
       "3   2.484584e-08  0.000001  \n",
       "4   2.484584e-08  0.000001  \n",
       "..           ...       ...  \n",
       "67  2.484584e-08  0.000001  \n",
       "68  2.484584e-08  0.000001  \n",
       "69  2.484584e-08  0.000001  \n",
       "70  2.484584e-08  0.000001  \n",
       "71  2.484584e-08  0.000001  \n",
       "\n",
       "[72 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd0c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microsimKernel",
   "language": "python",
   "name": "microsimkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
